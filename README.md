---
title: DÃ©tection d'Ã©motions faciales
emoji: ğŸ˜Š
colorFrom: blue
colorTo: red
sdk: docker
app_port: 7860
pinned: false
---

# Reconnaissance d'Ã©motions faciales â€” RAF-DB

SystÃ¨me de reconnaissance d'Ã©motions faciales entraÃ®nÃ© sur le dataset **RAF-DB** (Real-world Affective Faces Database). Le modÃ¨le classifie 7 Ã©motions Ã  partir d'images de visages via du **transfer learning VGG16**, exposÃ© par une **API REST FastAPI** et consommable via deux interfaces : une page web vanilla JS et un chatbot Streamlit interactif.

---

## Ã‰motions reconnues

| Label | Ã‰motion   | Emoji |
|-------|-----------|-------|
| 1     | Surprise  | ğŸ˜®    |
| 2     | Peur      | ğŸ˜¨    |
| 3     | DÃ©goÃ»t    | ğŸ¤¢    |
| 4     | Joie      | ğŸ˜„    |
| 5     | Tristesse | ğŸ˜¢    |
| 6     | ColÃ¨re    | ğŸ˜     |
| 7     | Neutre    | ğŸ˜    |

---

## RÃ©sultats

- **PrÃ©cision globale (weighted) : ~82.4%**
- Joie : F1 = 0.93 (classe majoritaire â€” 1 185 samples test)
- DÃ©goÃ»t : F1 = 0.41 (classe minoritaire â€” 74 samples test)

### Courbes d'entraÃ®nement

![Accuracy](results/accuracy.png)

### Matrice de confusion

![Matrice de confusion](results/matricec.png)

### Distribution des Ã©motions

![Distribution](results/distribution_emotions.png)

---

## Architecture du modÃ¨le

```
VGG16 (ImageNet, toutes les couches dÃ©gelÃ©es)
    â””â”€â”€ Flatten
    â””â”€â”€ Dense(512, relu) + BatchNormalization
    â””â”€â”€ Dropout(0.5)
    â””â”€â”€ Dense(7, softmax)
```

- **Input :** images RGB 100Ã—100
- **Optimizer :** Adam (lr=1e-4)
- **Loss :** sparse_categorical_crossentropy
- **Callbacks :** EarlyStopping (patience=8), ReduceLROnPlateau

---

## Structure du projet

```
raf-db/
â”œâ”€â”€ app_fastapi.py        # API FastAPI principale (inference)
â”œâ”€â”€ app.py                # API Flask alternative
â”œâ”€â”€ streamlit_app.py      # Chatbot Streamlit (camÃ©ra + upload fichier)
â”œâ”€â”€ index.html            # Frontend vanilla JS
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ model_raf.h5      # ModÃ¨le entraÃ®nÃ© (196 MB, Git LFS)
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ raf transfert VGG16.ipynb   # EntraÃ®nement principal (meilleurs rÃ©sultats)
â”‚   â”œâ”€â”€ raf transfert.ipynb
â”‚   â”œâ”€â”€ raf simple.ipynb
â”‚   â”œâ”€â”€ raf augmentation.ipynb
â”‚   â””â”€â”€ raf poid.ipynb
â”‚
â”œâ”€â”€ results/              # Graphiques et matrices de confusion
â”œâ”€â”€ samples/              # Images de test (dÃ©mo)
â”œâ”€â”€ test/
â”‚   â””â”€â”€ testgpu.py        # VÃ©rification GPU TensorFlow
â””â”€â”€ DATASET/              # RAF-DB (bind-mount, non versionnÃ©)
    â”œâ”€â”€ train/            # 12 271 images, sous-dossiers 1â€“7
    â””â”€â”€ test/             # 3 068 images, sous-dossiers 1â€“7
```

---

## DÃ©marrage rapide

### 1. PrÃ©requis

- Docker + [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)
- VS Code avec l'extension **Dev Containers**
- Dataset RAF-DB placÃ© dans `c:/ProjetIA/raf-db/DATASET`

### 2. Ouvrir dans le devcontainer

```bash
# Cloner le dÃ©pÃ´t
git clone <url-du-repo>
cd raf-db

# VS Code : "Reopen in Container"
# L'image nvcr.io/nvidia/tensorflow:25.02-tf2-py3 sera utilisÃ©e (GPU activÃ©)
```

### 3. Lancer l'API

```bash
python app_fastapi.py
# API disponible sur http://localhost:5000
```

### 4. Interface web (vanilla JS)

Ouvrir `index.html` directement dans le navigateur (l'API doit tourner).

### 5. Chatbot Streamlit

```bash
pip install streamlit requests
streamlit run streamlit_app.py
# Disponible sur http://localhost:8501
```

Le chatbot propose deux modes d'entrÃ©e au dÃ©marrage :
- **ğŸ“· CamÃ©ra** â€” prise de photo en direct
- **ğŸ“ Fichier** â€” chargement d'une image depuis le PC (JPG, PNG, WEBP)

AprÃ¨s chaque analyse, des boutons **Continuer** (mÃªme mode) et **Changer de mode** sont affichÃ©s sous le rÃ©sultat.

---

## API REST

### `POST /predict`

Envoie une image et reÃ§oit les Ã©motions dÃ©tectÃ©es.

**Request :** `multipart/form-data` avec un champ `file` (image JPEG/PNG)

**Response :**
```json
{
  "faces_detected": 2,
  "predictions": [
    {
      "emotion": "Joie",
      "confidence": 91.3,
      "box": { "x": 120, "y": 45, "w": 80, "h": 80 }
    },
    {
      "emotion": "Neutre",
      "confidence": 78.5,
      "box": { "x": 300, "y": 60, "w": 75, "h": 75 }
    }
  ]
}
```

**Exemple curl :**
```bash
curl -X POST http://localhost:5000/predict \
  -F "file=@photo.jpg"
```

### `GET /test`

```json
{ "status": "ok" }
```

---

## Pipeline d'infÃ©rence

1. DÃ©codage de l'image uploadÃ©e
2. DÃ©tection des visages via **Haar Cascade OpenCV** (`detectMultiScale(gray, 1.1, 5)`)
3. Redimensionnement de chaque visage en 100Ã—100 + normalisation VGG16
4. InfÃ©rence batch (un seul appel `model.predict()`)
5. Retour de l'Ã©motion + confiance + coordonnÃ©es pour chaque visage

---

## Environnement technique

| Composant      | Version                              |
|----------------|--------------------------------------|
| Image Docker   | `nvcr.io/nvidia/tensorflow:25.02-tf2-py3` |
| Python         | 3.x                                  |
| TensorFlow     | 2.x (GPU)                            |
| NumPy          | < 2.0.0 (1.26.4)                     |
| SciPy          | < 1.13.0                             |
| OpenCV         | < 4.10 (headless)                    |
| FastAPI        | latest                               |
| Streamlit      | latest                               |

---

## Dataset

**RAF-DB** (Real-world Affective Faces Database) â€” dataset public de visages annotÃ©s en conditions rÃ©elles.

- Train : 12 271 images
- Test : 3 068 images
- 7 classes avec dÃ©sÃ©quilibre significatif (JoieÃ—16 vs Peur)

> Le dataset n'est pas inclus dans ce dÃ©pÃ´t. Il doit Ãªtre placÃ© dans `DATASET/` (bind-mount devcontainer).
